\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}

\DeclareMathOperator{\Range}{range}

\newtheorem{Problem}{Problem}

\begin{document}

\section{Problem Statement}

Fix a \(M \times k\) word-topic matrix \(A\).
Suppose there is a document with topic distribution \(t\).
We observe a vector \(w\) of word counts sampled from \(At\).
\begin{Problem}
    \label{Problem:InferTopics}
    Given the words \(w\), estimate the topic distribution \(t\).
\end{Problem}

\section{Algorithms}

We consider three algorithms to solve Problem~\ref{Problem:InferTopics}.

\subsection{\(\ell_2\) Projection}

Return the vector \(t\) which minimizes \(|w - A t|_2\).
In other words, orthogonally project \(w\) to \(w' \in \Range(A)\), and then make \(w' = A t\).

(TODO: double-check this part)
We can also think of this in terms of matrix inverses.
Let \(A^+\) be the Moore-Penrose pseudoinverse of \(A\): so \(A^+A = I_k\), and the Frobenius norm of \(|A^+|\) is as small as possible.
The \(\ell_2\) projection algorithm returns the topic vector \(A^+w\).

\subsection{Kleinberg-Sandler}

The Kleinberg-Sandler algorithm~\cite{kleinberg2004using} uses a different generalized inverse.
Let \(B\) be a matrix such that \(BA = I_k\), and try to minimize the maximum entry of \(B\).
If you want to think in terms of \(\ell_1\) and \(\ell_\infty\), we are minimizing \(|B|_{1,\infty} = \max_{|x|_1 = 1} |B x|_\infty\).

% TODO

\subsection{Maximum Likelihood}

% TODO

\section{Examples}

% TODO

\bibliography{bib}{}
\bibliographystyle{plain}

\end{document}
