\documentclass{article}
\setlength{\parindent}{0cm}
\setlength{\parskip}{5mm}
%\setlength{\topsep}{-1cm}
%\setlength{\itemsep}{-1cm}
\usepackage{graphicx}
\usepackage{amsmath,mathrsfs,amsthm,amssymb}
\title{TBD}
\author{}

\date{}

\newcommand{\mcf}{{\zeta}}

\begin{document}
\maketitle

\begin{abstract}

We 

\end{abstract}

\section{Introduction} \label{sec:intro}
\input{intro}

\section{Related work}
\input{relatedWork}
\section{LDA}
LDA is introduced in~\cite{Blei2003a} as a generative process. As a model it is widely applied in various domains involving a collection of data, for example text analysis, collaborative filtering, image retrieval, and bioinformatics. In this work we will adopt the language of text collections, and denote entities as 'word', 'document', and 'corpus', since they are more intuitive in discussing topic models.  

The basic idea is that there exist $k$ underlying latent topics. Each document is a mixture of the latent topics, where the topic mixture is drawn from a dirichlet distribution. More precisely, there are $n$ documents, $m$ words, and $k$ topics. The model has a $m\times k$ topic matrix $A$, where the $i$-th column $A_i$ specifies a multinomial distribution over the $m$ words for topic $i$. For a document $w$, we first choose $\theta$, its distribution over topics, which can take values in the $(k-1)$-simplex, and has the following Dirichlet distribution
\[
p(\theta|\alpha)=\frac{\Gamma(\sum_{i=1}^k\alpha_i)}{\Pi_{i=1}^k\Gamma(\alpha_i)}\theta_i^{\alpha_1-1}\cdots\theta_k^{\alpha_k-1}
\] 
$(\alpha_1,\ldots,\alpha_k)$ are parameters of the model. The number of words in document $w$ is sampled from $Poisson(l)$. For each word $w_i$, first choose a topic $z_i\sim Multinomial(\theta)$, and then choose the actual word $w_i\sim Multinomial(A_{z_i})$. In matrix form, there are the $m\times k$ topic matrix $A$, and $k\times n$ matrix $W$ whose columns are drawn from $Dirichlet(\alpha_1,\ldots,\alpha_k)$ i.i.d. The product $M=AW$ is the $m\times n$ term-document matrix where column $M_i$ is document $i$'s distribution over words. Document $i$ is generated by sampling words i.i.d from $Multinomial(M_i)$.

Plot the typical topics/documents for various parameters, discuss the structure of the data with different model parameters. (Introduce number of significant words/topics as a more intuitive summary of the data?)

\section{Data}
Since our focus is on how effectively the algorithms learn the model, we use synthetic datasets generated from the LDA model for a range of parameters. Our data generator takes in parameters
\begin{itemize}
	\item $n,m,k$, number of documents, words, and topics respectively
	\item $\alpha$, the parameter of the Dirichlet distribution of the documents' distributions over topics. In our experiments we work with symmetric Dirichlet distributions, where $\alpha_i=\ldots=\alpha_k=\alpha$
	\item $\beta$, in addtion to the original LDA model where the entire topic matrix $A$ is parameter of the model, we assume the columns of $A$ are sampled from a $m$ dimensional Dirichlet random variable. Again we work with symmetric Dirichlet where $\beta_1=\ldots=\beta_m=\beta$. 
	\item $l$, the Possoin parameter controlling the expected number of words in a document.
\end{itemize}
Intuitively the Dirichlet parameter $\alpha$ is a crude measure of the sparsity of the sampled distribution over topics. When $\alpha=1$, all points in the $k-1$ simplex have the same probability density. When $\alpha<1$, distributions that are more concentrated on a few topics are prefered by the Dirichlet. The same applies to $\beta$ and the topic's distribution over words. See figure~\ref{fig:beta_plots} for typical word distributions sampled from the Dirichlet distribution with various $\beta$'s as parameter. 
\setlength\tabcolsep{0.5pt}
\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{beta_plots.pdf}
	\caption{Plot of various distributions over words for different $\beta$. $m=1000$, each distribution is plotted along with its cdf after sorting the words by popularity. Refer to the y-axis on the right for the scaling of the distributions. In general, larger $\beta$ values yield flatter distributions.}
	\label{fig:beta_plots}
\end{figure*} 
\setlength\tabcolsep{6pt}
We computed the value $sig\_topic$ as a more straightforward measure of sparsity than $\alpha$. For a document with distribution $\theta$ over topics, $sig\_topic$ is the smallest $t$ such that sum of the $t$ heaviest components in $\theta$ is at least $0.8$. Intuitively, $sig\_topic$ is the number of significant topics of a document. For topic's distribution over words, we have a similar measure $sig\_word$ such that the $sig\_word$ heaviest words in that topic have an aggregate probability of at least $0.8$. Instead of using $\alpha$ and $\beta$, we use the average $sig\_topic$ and average $sig\_word$ to characterize our datasets. 

\section{Experiments}
The prediction task we use is as follows. For a corpus of documents, we randomly divide the documents into the training set and the testing set, where each document is put into the training set with probability $p_T$ independently. For each document in the testing set, we hold out a certain percentage $H$ of the distinct words in the document uniformly at random. The training set is given to the algorithms. After training, each algorithm gets the testing documents, and for each document predicts $s$ terms not in the observed part of the testing document. We use the precison of the $s$ predicted terms as the score of an algorithm on a specific testing document. The score of an algorithm on the corpus is its average score over all testing documents. In our experiments, we use $p_T=0.9, H=30\%,s=3$ as our parameters.

This prediction task is widely applied in practice, especially in online settings with large amount of user-generated data. A familiar example is the collaborative filtering system by which Netflix leverages the preferences of a large user population to recommend new films to a customer after observing his or her viewing history. For our purpose, the prediction task provides a more straightforward algorithmic measure than statistical measures such as perplexity and likelihood, which are commonly adopted in machine learning, but not applicable to algortihms that don't reconstruct a statistical model. 

For those algorithms that reconstruct a topic matrix $\hat{A}$ in the process, we also measure how close $\hat{A}$ and $A$ are. For each learned topic $A_i$ and real topic $A_j$, we compute $cosine(\hat{A}_i,A_j)$, then find a maximum matching between the learned topics and real topics according to the cosine similarities. We also carry out the above computation using total variation distance between distributions, and get same qualitative results between algorithms. 

\section{Algorithms}
We compare several algorithms on the prediction task
\subsection{Baseline.}
For each testing document, the baseline algorithm predict the most frequent word in the training documents that we haven't seen in this testing document so far.
\subsection{K Nearest Neighbors (KNN)}
As a benchmark, we also include the result of KNN on the prediction task. For parameter $k$, KNN finds the $k$ most similar training documents to a testing document, where similarity is defined as the cosine between the two documents as vectors in the word space. For a testing document, KNN predicts the most frequent word in its $k$ closest training documents that we haven't seen in the testing document. Notice baseline is just KNN with $k$ equals the number of training documents. 
\subsection{LSI}







\section{Experiment Results}

\section{Analysis}
\input{Analysis}
\section{Conclusion}

\bibliographystyle{plain}
\bibliography{di}

\end{document}
