
With the vast production of data comes a great desire
to understand and make use of its structure. One
avenue has been to use generative models to provide
insight.  Topic models, in particular,
have been quite popular to investigate documents
and an increasing number of other domains (See \cite{BleiCACM} for
a survey of this field.)

% The early history of these things...

A relevant, prehistorical, viewpoint is the concept of latent semantic
indexing which views a document as being about a subset of a larger
set of topics.  Topics are in turn are about (positive numbers) or not
about (negative numbers) certain words \cite{Papadimitriou1997}.  They
used this view to provide an algorithm based on linear algebra that
learned the structure of their model.  They also argued that this
model provided some insight into the wide applicability of principal
components analysis in data analysis. The algorithmic tool at the
core of their work (and principal components) was the singular value
decomposition.

Shortly thereafter, a probabilistic framework was provided by Puzicha
and Hofmann \cite{Hofmann04}.  They termed their method probabilisitc latent
semantic indexing (pLSI), and modelled topics as probability
distributions over words (no negative numbers in this description).
Documents were then presumed to be generated as a mixture of these
topics.  The large number of parameters in the PLSI model motivated
Blei, Ng and Jordan to provide a generative model for these parameters
as well; they provided the enormously influential Latent Dirichlet
Allocation(LDA) topic model \cite{Blei2003a}.

Both pLSI and LDA have had enormous impact, but remain challenging
both in terms of the appropriateness of the model and in terms of
algorithms fitting the model.  The evaluation of efficacy departed
from tradition in machine learning in that one evaluated the learned
parameters in terms of predicted probability of the corpus
(``perplexity'') rather than on the performance of some task. Indeed,
this perspective remains compelling.  For example, in
\cite{BleiCBA} the evaluation of new algorithms still proceeds very
much along these internal measures.  This has merits in that one goal
of topic modelling is to find something ``interesting'', for which
performance is difficult to define.  Still, the form of the measures
suggested in the literature can be very complex.  See, for example,
the definition of ``free energy'' in \cite{BleiCBA}.
 
This complexity can make it very difficult to see when such models are
appropriate or whether the algorithms are effectively learning the
model.  We repeat for emphasis that there are {\em two issues here}:
the applicability of the model, and the effectiveness of the
algorithm.  Neither is easy to evaluate in engineering the methods
for data analytics.

We seek to provide insight into the performance of algorithms and to
some extent understand the application to data.  In contrast to the
tendencies in topic modelling \cite{BleiCBA}, we wish to provide simple
methods for evaluation. In this paper, we primarily study the
effectiveness of algorithms for discerning LDA models {\em when we
know} they apply; for data generated by the model.

%%   We derive some
%% insights and discuss some real world datasets.  

An effective model should lead to good performance on prediction. We
thus return to a simple prediction task as a primary figure of merit.
Topic models, particularly LDA, generate documents word by word.
Thus, a fair evaluation is to use a training set of documents and a
subset of a document to predict an unseen word.  This task, as we
show, entails both learning the topics and the distribution of topics
in a particular document.  The ``one-more'' (or ``$k$-more'') word
task is central to our study.  This task also seems to have natural
applications which we touch upon later.

A feature of this approach is that one can compare to methods not
based on modelling.  Thus, we can compare performance based on
modelling topics to standard learning methods.  In particular, we
include nearest neighbor methods which have been found to be highly
effective in the literature and in our experiments.

We found that for simulated data that the algorithms widely used for
learning LDA appear to be reasonably effective, but traditional
algorithms based on LSI are, in fact, {\em more effective.}  We
further engineered a new method combining LSI, $k$-means, and the
inference part of LDA.  This method is more robust than
LSI and has the best performance of any method we tested.

We characterize the instances that we can learn, the instances where
knowing the parameters give power (even when we fail to learn them),
and finally where the model fails to give any predictive advantage at
all even cheating.  The resulting characterization is unsuprising:
simply measure the independence or lack thereof of the selection of
words in a particular document is key.  On the other hand, taking
advantage of such simple measures has been key to giving provable
algorithms for numerous mixture problems ranging from learning
mixtures of Gaussians \cite{VempalaW04}, to hidden markov problems
\cite{MosselRoch}, to recent provable method for learning topic models
\cite{Arora2012} and even the LDA model \cite{AnandLDA}.  Indeed, we must
first recognize order before distinguishing it from chaos.

Finally, we test the methods on some real data sets.  Here, we find
that topic models do distinguish from chaos. Indeed, we find that the
LDA algorithms do better than both LSI and our engineered methods
which dominate on the simulated data.  This is quite interesting, and
gives gives testament to the notion that topic modellers have worked
extensively with real data; model or not.  Still, the most effective
algorithms in our experiments by far for real datasets is the nearest
neighbor algorithms.

We proceed in the sequel by describing the LDA model in section ??, We
give some experiments for a range of parameters in section ??.  We
explore the border of where the algorithms and than even the topic
parameterization fail to provide performance. We explore real
datasets in section ??.   We conclude in section ??.





