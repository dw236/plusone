\subsection{Baseline.}
For each testing document, the baseline algorithm predicts $s$ unseen words that are most frequent in the training documents.
\subsection{K Nearest Neighbors (KNN)}
For parameter $k$, KNN finds the $k$ most similar training documents to a testing document, where similarity is defined as the cosine between the two documents as vectors in the word space. For a testing document, KNN predicts the $s$ unseen words that are most frequent in its $k$ closest training documents. Notice baseline is just KNN with $k$ equals the number of training documents. 
\subsection{LDA algorithm}
It is $NP$-hard to find the maximum likelihood fit to the LDA model, so in practice, the prevailing approach to learn LDA model is local search. We adopted the algorithm implemented by Blei \cite{LDAcode} based on variational EM (see \cite{BleiLDA}). We also used an implementation \cite{GibbsLDA} based on Gibbs sampling. The two algorithms give close results on the prediction task for representative datasets. We didn't use the Gibbs sampling implementation on all datasets since it is much slower. In our discussion, LDA algorithm refers to the first implementation based on variational EM.

The LDA algorithm has two procedures,{\em "estimation"} and {\em "inference"}. {\em "estimation"} takes a collection of documents as input, and output estimated model parameters for the corpus, in particular the estimated term-topic matrix. {\em "inference'} takes a collection of documents as well as a LDA model, and outputs a infered topic mixture for each document. For our prediction task, we use the estimated term-topic matrix of the training set and the topic mixture for each testing document to estimate the most likely unseen words.

\subsection{LDAT and LDAC}
For generated LDA datasets, we have two "cheating" algorithms as benchmarks. LDAT knows the real term-topic matrix $A$ of the model. We skip {\em "estimation"}, and use the real $A$ at {\em "inference"}. LDAC knows the real real term-topic matrix $M$, and for each testing document, just predict the most likely unseen word according to the real distribution on words of the specific testing document. LDAC is supposedly the best we can do given sampling noises.

\subsection{LSI}
In the LDA model, we have $M=AW$, where $A$ and $W$ are the word-topic matrix and topic-document matrix respectively. Each column of M is a probability distribution on words. The $i$th document we observe is a set of i.i.d samples from the distribution $M_i$, and we have the observed word distribution $\hat{M}_i$. In the word space $\mathbb{R}^m$, all colunms of $M$ lie in the $k$-dimensional subspace spanned by the $k$ topics $A$. The sampled document $\hat{M}_i$ will be a noisy version of $M_i$, so in the word space $\mathbb{R}^m$, the points in $\hat{M}$ will be scattered close to the subspace of $A$.

LSI works by computing the best $k$-dimensional approximation to $\hat{M}$ of the training documents
\begin{align*}
&min_{U,\Sigma,V}\norm{\hat{M}-U\Sigma V}_F\\
\text{such that } &U\in \mathbb{R}^{m\times k},V\in \mathbb{R}^{k\times n}\text{  } orthonormal, \Sigma\in \mathbb{R}^{k\times k}\text{ }diagonal.
\end{align*}
The optimal $U,S,V$ are computed using singular value decomposition (SVD) of $\hat{M}$. LSI is not a statistical model in that the $U$ and $V$ matrices contains negative entries. The subspace spanned by columns of $U$ serve as a approximation of the subspace of $A$. Notice if we carry out SVD on $M$, the subspace of $U$ will be exactly the same as subspace of $A$. For a testing document $w$, we find its projection $\hat{w}=UU^Tw$ on the subspace of $U$, and predict $s$ unseen words with largest entries in $\hat{w}$.

\subsection{Projector}
Projector is our new algorithm that builds upon LSI, and reconstructs a term-topic probability matrix $\hat{A}$. The motivation is that SVD is computationally more efficient than LDA algorithm, and has a clear geometric interpretation, but doesn't recover the topics as distributions of words. We aim to start from the subspace computed by SVD, and use some straightforward operation to construct the topics. Our algorithm is based on geometric intuition of the documents as points in the high dimensional word space. The algorithm is as follows
\begin{description}
	\item[Input] $\hat{M}$: observed distributions of training documents, $k$: number of topics, $\delta$: algorithm parameter
	\item[Shift] Shift the training documents to be centered at the origin.\\
			     $center=\frac{1}{n}\sum_{i=1}^n\hat{M}_i$\\
			     $\hat{M}_i=\hat{M}_i-center\qquad \forall i=1,\ldots,n$
	\item[SVD] Compute the U, the best $(k-1)$-dimensional approximation of $\hat{M}$\\
                                 Project all $\hat{M}_i$'s to the subspace $U$, denote $V_i$ as the projections.
	\item[Clustering] Use k-means to cluster the $V_i$'s into $k$ clusters, where in the k-means algorithm the distance between two points $x,y$ is defined as $1-cos(x,y)$.\\
				      Let $C_1,\ldots,C_k$ be the centers of the $k$ clusters (center in the sense as in euclidean distance).
	\item[Scale] Scale $C_1,\ldots,C_k$ by the smallest common scalar so that $\delta n$ of $V_1,\ldots,V_n$ are contained in the hull with $C_1,\ldots,C_k$ as vertices. 
	\item[Whitening] Make all $C_i$ distribution over words: $C_i=C_i+center$, truncate the negative entries in $C_i$ to be $0$, normalize $C_i$ so the sum of entries is $1$.\\
                                            Return $\hat{A}_i=C_i$ be the recovered topics.
\end{description}
We illustrate how our algorithm works using the following visualization on two datasets with $k=3$ topics. Notice after the {\em Shift} step, we want to find the best $(k-1)$-dimensional subspace since the columns from topic-document matrix are from the $(k-1)$-simplex.
\begin{figure*}
     \begin{center}

        \subfigure[$\alpha=0.1,\beta=0.25,k=3$]{

            \includegraphics[width=0.45\textwidth]{c1.jpg}
        }
        \subfigure[Algorithm illustration with $\delta=0.8$]{

           \includegraphics[width=0.45\textwidth]{c2.jpg}
        }\\ 
        \subfigure[$\alpha=0.8,\beta=0.25,k=3$]{

            \includegraphics[width=0.45\textwidth]{b1.jpg}
        }
        \subfigure[Algorithm illustration with $\delta=0.8$]{

            \includegraphics[width=0.45\textwidth]{b2.jpg}
        }

    \end{center}
    \caption{Illustration of Projector. The left figures are the $V_i$'s after the SVD step. In the right figures, the black 'o's at the ends of dotted lines are the real topic, black $\times$ are the $C_i$'s before scaling, black $\diamond$ are $C_i$'s after scaling, and black $\Box$ are the recovered $\hat{A}_i$'s. All points in the plot are after shifting and projected on the SVD subspace. }
   \label{fig:subfigures}
\end{figure*}


We use estimated $\hat{A}$ and the inference procedure of the LDA algorithm to predict words for testing documents. We use the inference procedure of LDA since LDAT also uses it, and then we can attribute the performance difference between LDAT and Projector to the quality of $\hat{A}$ compared to the real topics.   
