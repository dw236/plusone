\documentclass{article}
\setlength{\parindent}{0cm}
\setlength{\parskip}{5mm}
%\setlength{\topsep}{-1cm}
%\setlength{\itemsep}{-1cm}
\setlength{\oddsidemargin}{-.3in}
\setlength{\evensidemargin}{-.3in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}

\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\headsep}{0pt}
%% \setlength{\topskip}{0pt}
%% \setlength{\topmargin}{0pt}
%% \setlength{\topsep}{0pt}
%% \setlength{\partopsep}{0pt}

\usepackage{graphicx}
\usepackage{amsmath,mathrsfs,amsthm,amssymb}
\usepackage{subfigure}
\title{Do LDA algorithms learn LDA models? }
\author{Andrew Gambardella, Victor Huang, Satish Rao, Di Wang, Chenyu Zhao}

\date{}

\newcommand{\mcf}{{\zeta}}
\newcommand{\norm}[1]{\lVert#1\rVert}

\begin{document}
\maketitle

\begin{abstract}
The Latent Dirichlet Allocation topic model has received enormous
attention as a method for extracting insight from documents and a wide
variety of other data sources.  Two interesting aspects in its application
are the effectiveness of learning algorithms, and the applicability to real
data.  In this paper, we primarily study the former but also apply some
of what we learn to the latter.

For a common view, we test with on the task of predicting a dropped
out word in the document.  This task is both natural for the LDA model
which successively generates words in a document and is of independent
interest. 

%% By dropping all occurences of test words, the task
%% is both more difficult and more applicable.

We generate data according to the LDA model and then evaluate a range
of algorithms, including one of our invention, on this generated data.
We characterize features of the generated data to help predict the
peformance of these algorithms; in particular, typical number of
topics in a document, and the typical number words in a topic.  We
give a simple analysis using these parameters which indicates 
when performance should drop off and verify that it is predictive.
We also examine the use of a slight modification of the Chi squared
statistic to predict the benefit of topic models. 

The new algorithm is based on a geometric view of topic models rather
than an inference based view of finding the optimal model.  This leads
to a simple algorithm that runs extremely quickly compared to the best
inference based methods available (e.g., the optimized Gibbs sampling
based topic learning method in MALLET, Minmo et.al.)  The performance
on generated data is comparable to the best inference based methods in
both predicting dropout words and learning the underlying topic models.

We note that our algorithms are more similiar to recent algorithms
that were proved to learn topic models.  

%%NOTE: engineered easily, flexible?

%% We find that the standard algorithms for LDA are quite poor compared
%% with principal components methods (latent semantic indexing or LSI)
%% and our new method with data generated from their model.  In contrast,
%% the LDA methods do perform better than principal components methods on
%% real world data, though, fall far short of nearest neighbor methods.
%% While the goal of topic modelling is not nessarily prediction, we
%% believe this study provides useful information about their use.

%% We note that it is ironic that the algorithms for LDA fail to beat LSI based
%% methods on their "LDA" data  but do on real world data.

\end{abstract}

\section{Introduction} \label{sec:intro}
\input{intro}

\section{Related work}
\input{relatedWork}
\section{LDA model} \label{sec:ldamodel}
LDA is introduced in~\cite{Blei2003a} as a generative process. As a model it is widely applied in various domains sucha as information retrieval, collaborative filtering, vision, and bioinformatics. In this work we will adopt the language of text collections, and denote entities as 'word', 'document', and 'corpus', since they give intuition in discussing topic models.  

The basic idea is that there exist $k$ underlying latent topics. Each document is a mixture of the latent topics, where the topic mixture is drawn from a dirichlet distribution. More precisely, there are $n$ documents, $m$ words, and $k$ topics. The model has a $m\times k$ word-topic matrix $A$, where the $i$-th column $A_i$ specifies a multinomial distribution on the $m$ words for topic $i$. For a document $w$, we first choose $\vec{\theta}$, its distribution over topics, which can take values in the $(k-1)$-simplex, and has the following Dirichlet distribution
\[
p(\theta|\vec{\alpha})=\frac{\Gamma(\sum_{i=1}^k\alpha_i)}{\Pi_{i=1}^k\Gamma(\alpha_i)}\theta_i^{\alpha_1-1}\cdots\theta_k^{\alpha_k-1}
\] 
where $\vec{\alpha}$ is parameter of the model. The number of words in document $w$ is sampled from $Poisson(l)$. For each word $w_i$, a topic $z_i\sim Multinomial(\vec{\theta})$ is chosen, then the actual word $w_i\sim Multinomial(A_{z_i})$ is generated. Equivalently, in matrix form, there are the $m\times k$ word-topic matrix $A$, and $k\times n$ topic-document matrix $W$ whose columns are drawn i.i.d from $Dirichlet(\vec{\alpha})$. The product $M=AW$ is the $m\times n$ term-document matrix where column $M_i$ is document $i$'s distribution on words. Document $i$ is generated by sampling words i.i.d from $Multinomial(M_i)$. We are interested in the case where $A$ is of full rank, since if the columns of $A$ are not independent, intuitively it means there exists some document which is covered completely by a set of topics $I$, but at the same time also completely covered by another set of topics $J$ which is disjoint from $I$. In our experiments, the randomly generated $A$ matrices are almost always of full rank. 

\section{Data} \label{sec:data}
Since our focus is on how effectively the algorithms learn the model, we use synthetic datasets generated from the LDA model for a range of parameters. Our data generator takes in parameters
\begin{itemize}
	\item $n,m,k$, number of documents, words, and topics respectively
	\item $\alpha$, the Dirichlet parameter for generating documents' distributions over topics as in the LDA model. In our experiments we work with symmetric Dirichlet distributions, where $\alpha_i=\ldots=\alpha_k=\alpha$
	\item $\beta$, we generate the columns of word-topic matrix $A$ from a $m$ dimensional Dirichlet distribution with parameter $\vec{\beta}$. Again we work with symmetric Dirichlet where $\beta_1=\ldots=\beta_m=\beta$. 
	\item $l$, the Poisson parameter controlling the expected number of words in a document.
\end{itemize}
Intuitively the Dirichlet parameter $\alpha$ is a crude measure of the sparsity of the sampled distribution over topics. When $\alpha=1$, all points in the $k-1$ simplex have the same probability density. When $\alpha<1$, distributions that are more concentrated on a few topics are prefered by the Dirichlet. The same applies to $\beta$ and the topic's distribution on words. See figure~\ref{fig:beta_plots} for typical word distributions sampled from the Dirichlet distribution with various $\beta$'s as parameter. 
\setlength\tabcolsep{0.5pt}
\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{beta_plots.pdf}
	\caption{Plot of distributions on words for various $\beta$. $m=1000$, each distribution is plotted along with its cdf after sorting the words by popularity. Refer to the y-axis on the right for the scaling of the distributions. In general, larger $\beta$ values yield flatter distributions.}
	\label{fig:beta_plots}
\end{figure*} 
\setlength\tabcolsep{6pt}
To help understand the dataset, we compute the values $sig\_topic$ and $sig\_word$. For a document with distribution $\vec{\theta}$ over topics, $sig\_topic$ is the smallest $t$ such that the union of the $t$ heaviest topics in $\vec{\theta}$ has an aggregate probability of at least $0.8$. Intuitively, $sig\_topic$ is the number of significant topics of a document. Analogously, for a topic's distribution over words, $sig\_word$ is the smallest number of most popular words with an aggregate probability of at least $0.8$.Instead of using $\alpha$ and $\beta$, we use the average $sig\_topic$ and average $sig\_word$ to characterize our datasets. 

We also have collected some real world datasets.  We used the
Classic-3 datasets \cite{Classic3}, a corpus from the Associated Press
\cite{AP} pruning stop words and infrequent words, and a bag of words
dataset from UC Irvine\cite{KosNIPS}.  


\section{Experiments}
\subsection{Prediction task}
For a corpus of documents, we randomly divide the documents into the training set and the testing set, where each document is put into the training set with probability $p_t$ independently. For each document in the testing set, we hold out a certain percentage $H$ of the distinct words in the document uniformly at random. The training set is given to the algorithms. After training, each algorithm gets the testing documents, and for each document predicts $s$ terms not in the observed part of the testing document. We use the precison of the $s$ predicted terms as the score of an algorithm on a specific testing document. The score of an algorithm on the corpus is its average score over all testing documents. In our experiments, we use $p_t=0.9, H=30\%,s=3$ as our parameters.

This prediction task is widely applicable in practice, especially in
online settings with a large amount of user-generated data. A familiar
example is the collaborative filtering system by which Netflix
leverages the preferences of a large user population to recommend new
films to a customer after observing his or her viewing history. For
our purpose, the prediction task provides a more straightforward
algorithmic measure than statistical measures such as perplexity and
likelihood, which are commonly adopted in machine learning, but not
applicable to algortihms that don't reconstruct a statistical model.

\subsection{Recovery task}
For the algorithms that reconstruct a topic matrix $\hat{A}$ in the
process, we also measure how close $\hat{A}$ and $A$ are. For each
learned topic $\hat{A}_i$ and real topic $A_j$, we compute
$cos(\hat{A}_i,A_j)$, then find a maximum matching between the learned
topics and real topics. We evaluate the average cosine similarity
between the matched real and learned topics. We also carry out the
above computation using total variation distance between
distributions, and get same qualitative results between algorithms.

\section{Algorithms}
\label{sec:algs}
\input{Alg}
\section{Experiment Results}
\label{sec:results}
\input{result}

\section{Analysis}
\label{sec:analysis}
\input{Analysis}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we made progress toward understanding the performance
of algorithms on data generated by the LDA model.  Using our
prediction task, we were able to find an improved algorithm for this
task.  This prediction task itself may be a reasonable candidate for
practitioners to use to evaluate algorithms that are trying to find
interesting topics. We will provide our framework either in MALLET
\cite{McCallumMALLET} or separately for this purpose.

We provided some rules of thumb for when algorithms can make
use of topic structure in generated data.  It is important
to see if these rules of thumb extend to real world datasets.
We are working on this aspect currently.

We also have begun to extend our framework to work
with hierarchical LDA, and will explore that 
in future work. 

\bibliographystyle{plain}
\bibliography{di}
\section{Appendix}
\input{appendix}
\end{document}
