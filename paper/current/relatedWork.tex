The most closely related work in theory are the aforementioned
algorithms provided by Arora et.al. \cite{Arora2012} which learn pLSI under
certain assumptions, and the breakthrough by Anandkumar\cite{AnandLDA} for LDA.
There is a long literature in learning mixtures of distributions in
statistics and more recently in theoretical computer science. See
\cite{MoitraValiant} for a recent breakthrough on learning mixtures of
Gaussians and a discussion of this field.

There is ample work describing methods for optimizing
LDA \cite{BleiCACM}.  Again, recent examples is described
in \cite{BleiCBA}.

There is also significant work on extending LDA and topic models to
better model data as well as to augment them with other types of
information.  These are discussed in \cite{BleiCACM}.  The
hierarchical LDA model \cite{BleiCM} is perhaps the most relevant.
This model is based on the chinese restuarant process where new
customers arrive and chose to join a table or create a new one.  This
process can be used to create a hierarchical structure of documents
and topics. The authors argue that this is a better model for real
data and we tend to agree.  Still, we began by understanding the
simpler model which continues to have wide application.  While we have
included the hLDA generator is included in our framework, the
implementations distributed by the authors are not yet stable within
our system.

To some extent, we seek to provide an infrastructure to evaluate topic
modelling. We should point out that the machine learning community has
been active at producing infrastructure for evaluation.  See for
example, MLComp \cite{MLComp}. A particularly related java based
infrastructure for topic modelling is \cite{McCallumMALLET}.  We plan to test their
implementations in the final version of this paper.
