In this paper, we empirically study standard algorithms for topic
modelling using Latent Dirichlet Allocation. We also develop geometry
based algorithms for learning the Latent Dirichlet Allocation (LDA)
which are as effective as inference based methods while being more
efficient. In particular, our most effective algorithm uses a novel
combination of dimension reduction, projection, $k$-means, and a
scaling procedure.

We argue that the evaluation of topic modelling should include a
prediction task that is closely aligned with the model. LDA, in
particular, generates the words in a document; thus, predicting
missing words is appropriate for the evaluation of the LDA model, both
when applied to a corpus and for genarated data.  Moreover, this task
is useful for recommendation systems, keyword suggestion, tag
prediction, etc.  For practitioners, performance on this task may be
more useful than internal measures such as perplexity or indirect
methods where classification methods are applied on  learned topic
models.

Finally, to empirically study algorithms, one needs to understand
properties of datasets.  We thus provide an analysis that predicts
whether a corpus can be effectively modelled.  For generated datasets,
the prediction is based on the diversity of topics in a typical
document, and the concentration of words in a topic.

%% We also study the empirical use of standard statistics as predictors
%% for the learnability of a model.



