\documentclass{sig-alternate}
\pdfpagewidth=8.5in
\pdfpageheight=11in
%\documentclass{article}
%\usepackage{amsmath,mathrsfs,amsthm,amssymb}
\setlength{\parindent}{0cm}
\setlength{\parskip}{5mm}
%\setlength{\topsep}{-1cm}
%\setlength{\itemsep}{-1cm}
%\setlength{\oddsidemargin}{-.3in}
%\setlength{\evensidemargin}{-.3in}
%\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{8in}

\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\headsep}{0pt}
%% \setlength{\topskip}{0pt}
%% \setlength{\topmargin}{0pt}
%% \setlength{\topsep}{0pt}
%% \setlength{\partopsep}{0pt}

\usepackage{graphicx}

\usepackage{subfigure}
\title{Alternatives for engineering and evaluating algorithms for LDA}
\author{Di Wang,  Victor Huang, James Cook, Andrew Gambardella,  Chenyu Zhao, Satish Rao
\thanks{UC Berkeley}
}

\date{}

\newcommand{\mcf}{{\zeta}}
\newcommand{\norm}[1]{\lVert#1\rVert}

\begin{document}
\maketitle

\begin{abstract}

\input{kdd-abstract.tex}

\end{abstract}

\section{Introduction} \label{sec:intro}
\input{kdd-intro}

\section{Related work}
\input{relatedWork}
\label{sec:related}

%\input{result}
\section{LDA model} \label{sec:ldamodel}
LDA was introduced in~\cite{Blei2003a} as a generative process. In
this work we will adopt the language of text collections, and denote
entities as 'word', 'document', and 'corpus', since they give
intuition in discussing topic models.

We begin with $ n $ documents, $ m $ words, and $ k $ latent topics. Each
document is a mixture of the latent topics $ \theta $, where the mixture is
drawn from a Dirichlet distribution with parameter $\vec{\alpha}$. 

\[
p(\vec{\theta}|\vec{\alpha})=\frac{\Gamma(\sum_{i=1}^k\alpha_i)}{\Pi_{i=1}^k\Gamma(\alpha_i)}\theta_i^{\alpha_1-1}\cdots\theta_k^{\alpha_k-1}
\] 

This results in an $n\times k$ document-topic matrix $ W $, where row $ i $ gives the parameters of the multinomial
distribution over topics for document $ i $. Similarly, we have a $ k \times m $ word-topic matrix $A$,
where the $j$-th row gives the parameters of the multinomial distribution
over words for topic $j$. The columns are drawn from a Dirichlet distribution with parameter
$ \vec{\beta} $.

For a document $w$, we first choose $\vec{\theta}$, which can take values in the $(k-1)$-simplex.
The number of words in document $w$ is sampled from $\operatorname{Poisson}(l)$. For each word $w_i$, a
topic $z_i\sim \operatorname{Multinomial}(\vec{\theta})$ is chosen, followed by the actual
word $w_i\sim \operatorname{Multinomial}(A_{z_i})$. The product $M=WA$ is the $n\times m$
document-word matrix where row $M_i$ is document $i$'s multinomial distribution over
on words.

Equivalently, document $i$ is generated by sampling words i.i.d from
$\operatorname{Multinomial}(M_i)$. We are interested in the case where $A$ is of full
rank, since if the row of $A$ are not independent, intuitively it
means there exists some document which is covered completely by a set
of topics $I$, but at the same time also completely covered by another
set of topics $J$ which is disjoint from $I$. In our experiments, the
randomly generated $A$ matrices are almost always of full rank.

\section{Data} \label{sec:data}

Since our focus is on how effectively the algorithms learn the model,
we use synthetic datasets generated from the LDA model for a range of
parameters. Our data generator takes in parameters
\begin{itemize}
	\item $n,m,k$, number of documents, words, and topics respectively
	\item $\alpha$, the Dirichlet parameter for generating documents' distributions over topics as in the LDA model. In our experiments we work with symmetric Dirichlet distributions, where $\alpha_i=\ldots=\alpha_k=\alpha$
	\item $\beta$, we generate the columns of word-topic matrix $A$ from a $m$ dimensional Dirichlet distribution with parameter $\vec{\beta}$. Again we work with symmetric Dirichlet where $\beta_1=\ldots=\beta_m=\beta$. 
	\item $l$, the Poisson parameter controlling the expected number of words in a document.
\end{itemize}
Intuitively the Dirichlet parameter $\alpha$ is a crude measure of the
sparsity of the sampled distribution over topics. When $\alpha=1$, all
points in the $k-1$ simplex have the same probability density. When
$\alpha<1$, distributions that are more concentrated on a few topics
are prefered by the Dirichlet. The same applies to $\beta$ and the
topic's distribution on words. See figure~\ref{fig:beta_plots} for
typical word distributions sampled from the Dirichlet distribution
with various $\beta$'s as parameter.  \setlength\tabcolsep{0.5pt}
\begin{figure}
%	\centering
	\includegraphics[width=3in]{beta_plots.pdf}
	\caption{Plot of distributions on words for various $\beta$. $m=1000$, each distribution is plotted along with its cdf after sorting the words by popularity. Refer to the y-axis on the right for the scaling of the distributions. In general, larger $\beta$ values yield flatter distributions.}
	\label{fig:beta_plots}
\end{figure} 
\setlength\tabcolsep{6pt}

To help understand the dataset, we compute the values $sig\_topic$ and
$sig\_word$. For a document with distribution $\vec{\theta}$ over
topics, $sig\_topic$ is the smallest $t$ such that the union of the
$t$ heaviest topics in $\vec{\theta}$ has an aggregate probability of
at least $0.8$. Intuitively, $sig\_topic$ is the number of significant
topics of a document. Analogously, for a topic's distribution over
words, $sig\_word$ is the smallest number of most popular words with
an aggregate probability of at least $0.8$.Instead of using $\alpha$
and $\beta$, we use the average $sig\_topic$ and average $sig\_word$
to characterize our datasets.


We also evaluate on a topic distribution which obeys a power 
law.  Instead of using the same $\alpha$ parameter for all
the topics, we vary it so that the expected usage.

We also evaluate the methods on some standard real world datasets.  We
used the Classic-3 datasets (Cran,Med,Cisi) \cite{Classic3}, a corpus
(AP) from the Associated Press \cite{AP} pruning stop words and
infrequent words, and a bag of words dataset (NIPS) from UC
Irvine\cite{KosNIPS}, MovieLens \cite{MovieLens}.

\section{Experiments}
\subsection{Prediction task}

For a corpus of documents, we randomly divide the documents into the
training set and the testing set, where each document is put into the
training set with probability $p_t$ independently. For each document
in the testing set, we hold out a certain percentage $H$ of the
distinct words in the document uniformly at random. The training set
is given to the algorithms. After training, each algorithm gets the
testing documents, and for each document predicts $s$ terms not in the
observed part of the testing document. We use the precison of the $s$
predicted terms as the score of an algorithm on a specific testing
document. The score of an algorithm on the corpus is its average score
over all testing documents. In our experiments, we use $p_t=0.9,
H=30\%,s=3$ as our parameters.

This prediction task is widely applicable in practice, especially in
online settings with a large amount of user-generated data. A familiar
example is the collaborative filtering system by which Netflix
leverages the preferences of a large user population to recommend new
films to a customer after observing his or her viewing history. For
our purpose, the prediction task provides a more straightforward
algorithmic measure than statistical measures such as perplexity and
likelihood, which are commonly adopted in machine learning, but not
applicable to algortihms that don't reconstruct a statistical model.

\subsection{Recovery task}
For the algorithms that reconstruct a topic matrix $\hat{A}$ in the
process, we also measure how close $\hat{A}$ and $A$ are. For each
learned topic $\hat{A}_i$ and real topic $A_j$, we compute
$cos(\hat{A}_i,A_j)$, then find a maximum matching between the learned
topics and real topics. We evaluate the average cosine similarity
between the matched real and learned topics. We also carry out the
above computation using total variation distance between
distributions, and get same qualitative results between algorithms.

\section{Algorithms}
\label{sec:algs}
\input{Alg}
\section{Experiment Results}
\label{sec:results}
\input{result}

\section{Analysis}
\label{sec:analysis}
\input{Analysis}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we made progress toward understanding the performance
of algorithms on data generated by the LDA model.  Using our
prediction task, we were able to find an improved algorithm for this
task as well as for learning the topics in a generated topic model.
This prediction task itself may be a reasonable candidate for
practitioners to use to evaluate algorithms that are trying to find
interesting topics.

We provided some rules of thumb for when algorithms can make use of
topic structure in generated data.  It is important to see if these
rules of thumb extend to real world datasets.  We are continuing on
this aspect currently.

We note that we tested Hierarchical LDA on our task
of predicting more words.  Admittedly, we had to
modify the algorithm for this task by sampling
many hLDA hidden states to estimate the word
distributions for the test documents.  We found
it to be substantially inferior to Gibbs LDA
on real datasets. 

Our framework is currently available on github and we are working to
make it user friendly. 

\bibliographystyle{plain}
\bibliography{di}
%\section{Appendix}
\input{appendix}



\end{document}

