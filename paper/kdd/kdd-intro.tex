
The Latent Dirichlet Allocation topic model has been tremendously
influential in the development of methods for analyzing documents and
other types of data.  Numerous algorithms for learning LDA, as
well as variations LDA, have been proposed; see \cite{BleiCACM} for
a recent survey.  The bulk of these methods
are based on inference techniques; they proceed by
learning the parameters using methods such as Gibbs sampling,
expectation-maximization, variational methods, and model modifications for faster
computation.  Evaluating these methods have typically proceeded
along one of two lines. One trains on a set of documents and evaluates
how well the resulting model predicts a test set, typically measured by
perplexity.  Another uses the model as a set of features for some
classification task (e.g. document classification) followed by the application of
some learning method (e.g. a support vector machine).

In this paper, we suggest an alternative to the inference approach in
learning topic models as well as introduce a different evaluation method. The
first yields an improved algorithm for learning LDA and the second
allows for better understanding of topic learning efficacy.

In terms of inference algorithms, we view model parameters as geometric objects.
For textual data, topic centers are simply points in the word space. This view
is very traditional in that data is seen as generated from points in space under
some noise model. This interpretation has long been associated with algorithms
such as nearest neigbors for classification and $k$-means for clustering. As we show
though, nearest neighbors and $k$-means are inferior to
present LDA inference algorithms for LDA generated data.  However, we show that
a combination of $k$-means with dimension reduction and a
scaling step produces an effective algorithm for this type of data.
For real world data, our methods remain just as effective as LDA inference algorithms
and more effective than $k$-means, but falls short of nearest neighbor
techniques (as do all topic modelling approaches.) We refer to
our algorithm as the Projector algorithm.  We note here that recent
work in \cite{AnandLDA} use linear algebraic methods, which can be
viewed to some extent as geometric algorithms, to give an algorithm
that provably learns the LDA parameters given a polynomial
number of examples and polynomial time.

We measure our performance on the task of predicting a set of dropped out
words in a document.  Topic models, in particular the LDA topic model,
generate documents on a word by word basis.  Thus, a model's effectiveness
for predicting a word in a document is both natural and easily interpreted. It also
allows for the comparison of any prediction method, whether it explicitly learns
the topic model or not.  We also note that this task has been well studied
in the field of recommender systems, where documents are people and words are products.



%% Finally, we note that some datasets are more difficult than others for
%% all the methods.  Rather than characterize this difficulty using the
%% arcane parameters of the LDA model, we suggest that using more natural
%% measures is more useful.  In particular, when a typical document
%% begins to be sufficiently diffuse in topics or topics have relatively
%% large support over words, it becomes difficult to improve over the
%% simple strategy of choosing the most frequent left out word.

Before proceeding, we note that this paper proposes an algorithm,
argues for more informative evaluations, and does experimental studies
of existing algorithms.  We feel these things go very much together.  The
proof for a methodology for algorithm development is, after all, an
effective algorithm.

We proceed with related work in section~\ref{sec:related}, a
description of the LDA model in section~\ref{sec:ldamodel}, the data
generation process in section~\ref{sec:data}, a description of the
algorithms we study in section~\ref{sec:algs}, the results of our
experiments in section~\ref{sec:results}, a bit of analysis in
section~\ref{sec:analysis}, and conclude in \ref{sec:conclusion}.

