
In this paper, we describe geometry based algorithms for learning the
Latent Dirichlet Allocation (LDA) are as effective as inference based
methods and can be far more efficient and easier to reason about.  We
posit that the evaluation of topic modelling should include a
prediction task that is closely aligned with the model. LDA, in
particular, is a generative model which successively predicts words.
Thus, a prediction task to predict missing words seems both fair in
the evaluation of LDA learning methods, both when applied to a corpus
and for genarated data, and could be quite useful, e.g., for
recommendation systems, keyword suggestion, tag prediction, etc. 

We also provide an analysis that predicts whether a corpus can be
modelled based on the diversity of topics in a typical document, and
the concentration of topics in the corpus.



